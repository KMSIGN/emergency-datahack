{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './data/track_1/'\n",
    "\n",
    "# гидро\n",
    "hydro_1day = pd.read_csv(directory + 'hydro_1day.csv', parse_dates=['date'])\n",
    "# hydro_coord = pd.read_csv(directory + 'hydro_coord.csv')\n",
    "\n",
    "# метео — обратите внимание на различие во временной детализации\n",
    "# meteo_3hours = pd.read_csv(directory + 'meteo_3hours.csv')\n",
    "# meteo_1day = pd.read_csv(directory + 'meteo_1day.csv')\n",
    "# meteo_1month = pd.read_csv(directory + 'meteo_1month.csv')\n",
    "# meteo_coord = pd.read_csv(directory + 'meteo_coord.csv')\n",
    "\n",
    "# справочники\n",
    "# reference_water_codes = pd.read_csv(directory + 'reference_water_codes.csv')\n",
    "# reference_horiz_visib = pd.read_csv(directory + 'reference_horiz_visib.csv')\n",
    "\n",
    "# тест и трейн\n",
    "train_df = pd.read_csv(directory + 'train.csv')\n",
    "test_df = pd.read_csv(directory + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 37.27it/s]\n"
     ]
    }
   ],
   "source": [
    "hydro_1day = pd.read_csv(directory + 'hydro_1day.csv', parse_dates=['date'])\n",
    "\n",
    "\n",
    "\n",
    "# разделяем код режимной группы\n",
    "water_code = pd.DataFrame(hydro_1day['water_code'].str.split(',', expand=True).fillna(0).astype(int))\n",
    "water_code.columns=[f'water_code_{i}' for i in range(5)]\n",
    "\n",
    "\n",
    "# добавляем в общий датасет\n",
    "hydro_1day = pd.concat([hydro_1day, water_code], axis=1)\n",
    "hydro_1day = hydro_1day.drop(['water_code'], 1)\n",
    "hydro_1day = hydro_1day.sort_values(by=['year', 'day'])\n",
    "hydro_1day = hydro_1day.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# удаляем не нужные фичи\n",
    "hydro_1day = hydro_1day.drop(['month'], 1)\n",
    "\n",
    "# запонняем пустые значения\n",
    "hydro_1day[['place', 'snow_height']] = hydro_1day[['place','snow_height']].fillna(0)\n",
    "hydro_1day.loc[hydro_1day['temp'].isna(), 'temp'] = hydro_1day['temp'].median()\n",
    "\n",
    "# нормализируем\n",
    "hydro_1day['place'] = hydro_1day['place'].astype(int)\n",
    "lb_place = LabelEncoder()\n",
    "hydro_1day['place'] = lb_place.fit_transform(hydro_1day['place'])\n",
    "hydro_1day[['stage_avg', 'stage_min', 'stage_max']] = hydro_1day[['stage_avg', 'stage_min', 'stage_max']].abs()\n",
    "\n",
    "\n",
    "# интерполируем данные для каждой станции\n",
    "columns_interpolate = ['stage_avg', 'stage_min', 'stage_max', 'temp', 'discharge', 'ice_thickness']\n",
    "stations = hydro_1day['station_id'].unique()\n",
    "for i in tqdm(stations):\n",
    "    for col in columns_interpolate:\n",
    "        hydro_1day.loc[hydro_1day['station_id'] == i, col] = hydro_1day.loc[hydro_1day['station_id'] == i, col].interpolate(limit_direction='both')\n",
    "        \n",
    "        \n",
    "hydro_1day['discharge'] = hydro_1day['discharge'].fillna(hydro_1day['discharge'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 8960/8960 [00:04<00:00, 1848.48it/s]\n"
     ]
    }
   ],
   "source": [
    "hydro_1day['real'] = 1\n",
    "empty_rows = []\n",
    "stations = hydro_1day['station_id'].unique()\n",
    "\n",
    "for index, i in tqdm(hydro_1day.groupby(['year', 'day'])):\n",
    "    for station in stations:\n",
    "        if station not in i['station_id'].values:     \n",
    "            row = i.iloc[0].copy()\n",
    "            row['station_id'] = station\n",
    "            empty_rows.append(row.values)\n",
    "            \n",
    "       \n",
    "empty_df = pd.DataFrame(empty_rows, columns=hydro_1day.columns)\n",
    "empty_df.iloc[:,4:] = 0\n",
    "hydro_1day = pd.concat([hydro_1day, empty_df], ignore_index=True).sort_values(by=['year', 'day'])\n",
    "# hydro_1day = hydro_1day.append(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_1day.to_csv(directory + 'hydro_1day_prep.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days_data = 14\n",
    "def get_data(year):\n",
    "    if year-1 in hydro_1day['year'].values:\n",
    "        part1 = hydro_1day[(hydro_1day['year']==year-1)].iloc[-num_days_data*len(stations):]    \n",
    "    else:\n",
    "        part1 = hydro_1day[(hydro_1day['year']==year)].iloc[-num_days_data*len(stations):]  \n",
    "        if (year-1)%4:\n",
    "            days = 365\n",
    "        else:\n",
    "            days = 366\n",
    "\n",
    "        part1['date'] = part1['date'] - pd.Timedelta(days=days)\n",
    "\n",
    "    part2 = hydro_1day[(hydro_1day['year']==year)].iloc[:num_days_data*len(stations)]\n",
    "    result = pd.concat([part1, part2])\n",
    "    result = result.drop(['date'], 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "def train_model(df):\n",
    "    days_X = []\n",
    "    days_Y = []\n",
    "    for index1, group in tqdm(df.groupby(['station_id','year'])):\n",
    "\n",
    "        data = get_data(year=index1[1])\n",
    "\n",
    "        for i, (index, row) in enumerate(group.iterrows()):\n",
    "            period = data.iloc[i:num_days_data*len(stations)+i]\n",
    "            period = period.values.flatten()\n",
    "\n",
    "            days_X.append(np.append(row.drop('ice_jam').values, period))\n",
    "            days_Y.append(row['ice_jam'])\n",
    "\n",
    "    rfc.fit(days_X, days_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df):\n",
    "    Y_pred = []\n",
    "    full_train_df = df.copy()\n",
    "    for index1, group in tqdm(full_train_df.groupby(['station_id','year'])):\n",
    "\n",
    "        data = get_data(year=index1[1])\n",
    "        \n",
    "        X_days = []\n",
    "\n",
    "        for i, (index, row) in enumerate(group.iterrows()):\n",
    "            period = data.iloc[i:num_days_data*len(stations)+i]\n",
    "            period = period.values.flatten()\n",
    "\n",
    "            full_train_df.loc[index, 'ice_jam_pred'] = rfc.predict([np.append(row.drop('ice_jam').values, period)])\n",
    "            \n",
    "            \n",
    "    return full_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(train_df, test_size=0.33, random_state=42, stratify=train_df['ice_jam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [00:06<00:00, 40.58it/s]\n"
     ]
    }
   ],
   "source": [
    "train_model(X_train)\n",
    "pred = predict(X_test)\n",
    "f1_score(pred['ice_jam'], pred['ice_jam_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [00:08<00:00, 30.87it/s]\n"
     ]
    }
   ],
   "source": [
    "train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(rfc, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
